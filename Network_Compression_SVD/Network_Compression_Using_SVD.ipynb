{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network compression using Singular Value Decomposition on weights\n",
    "In this jupyter notebook, I have tried to compress weights of Deep Neural Network of 5-Layers for MNIST dataset problem. I have used Singular Value Decomposition on the weights of each layer by selecting top 10 and 20 features in Singular matrix.\n",
    "\n",
    "With normal 5-Layer Deep Neural Network (Fully connected layers) the test accuracies on MNIST data was 98.5%\n",
    "\n",
    "With top 20 features of Singular matrix values of trained weights, I was able to get 95% accuracy on test data for trained model.\n",
    "\n",
    "Training the model again with top 20 features of Singular matrix of trained weights, I was able to get accuracy of around 98%. And the training time was smaller compared to original training, as the weights were reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--disable-cuda', action='store_true',\n",
    "                    help='Disable CUDA')\n",
    "parser.add_argument('--interval',metavar='N',default=1000)\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.disable_cuda and torch.cuda.is_available()\n",
    "#Is cuda is present?\n",
    "print(args.cuda)\n",
    "#Total number of GPU available\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load MNiSt data\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('/opt/e533/MNIST',\n",
    "                    train=True,\n",
    "                    download=False,\n",
    "                    transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "                    batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('/opt/e533/MNIST',\n",
    "        train=False,\n",
    "        transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline Model for training \n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 1024)\n",
    "        self.fc5 = nn.Linear(1024, 1024)\n",
    "        self.fc6 = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=F.relu(self.fc4(x))\n",
    "        x=F.relu(self.fc5(x))\n",
    "        x=self.fc6(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model=model.cuda()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data=data.view(-1,28*28)\n",
    "#         print(data.shape)\n",
    "        if torch.cuda.is_available():\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "        data,target = Variable(data),Variable(target)\n",
    "        # Clears the gradients of all optimized Variables\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.interval  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment model load and comment the training for loop, to save time from training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.301851\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.247325\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.057546\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.083936\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.073802\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.012690\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.063057\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.014990\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.013824\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.017595\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.047051\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.004162\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.006474\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.000473\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.007822\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.012595\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000530\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.001966\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.004107\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.001057\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.000951\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.002340\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.000273\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000024\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.007672\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.027583\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.000272\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.000544\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.000413\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.002367\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.003335\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.000497\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.059011\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.008199\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.000228\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.000107\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.003027\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.000089\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.001220\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.000070\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000093\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.000165\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.033853\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.000037\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.002272\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.001949\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.000088\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.001857\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 0.000001\n"
     ]
    }
   ],
   "source": [
    "#Load already available model\n",
    "# model.load_state_dict(torch.load(\"optimum_wts\"))\n",
    "\n",
    "#Uncomment and comment above code the followint code to get new weights \n",
    "\n",
    "for t in range(53):\n",
    "    train(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_test():\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "\n",
    "    for i,(data,target) in enumerate(test_loader):\n",
    "        data = data.view(-1,28*28)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "        data,target = Variable(data),Variable(target)\n",
    "        \n",
    "        y_pred = model(data)\n",
    "        \n",
    "        \n",
    "        test_loss += criterion(y_pred, target).data[0]\n",
    "        pred = y_pred.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.001f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / len(test_loader.dataset)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9853/10000 (98.5%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following are the accuracies of different D\n",
    "##### For D = 10 features the test accuracies vary from 67.9 % to 74.5 % \n",
    "##### For D = 20, the test accuracies vary from  94 % to 95 %\n",
    "##### For D = 30,40 the test accuracies vary from 97 % to 97.6%\n",
    "##### For D = 50, the test accuracies is 98.1%\n",
    "#### For D=1024, the test accuracies is 98.5% (base line accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Test_Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        super(Test_Model, self).__init__()\n",
    "        self.D = 20\n",
    "        \n",
    "        #weight1 and bias\n",
    "        params=model.state_dict()\n",
    "        \n",
    "        self.weight1 = Variable(params[\"fc1.weight\"].t())\n",
    "        self.bias1 = Variable(params[\"fc1.bias\"])\n",
    "        \n",
    "\n",
    "        \n",
    "        u,s,v = torch.svd(self.weight1)\n",
    "        u,s,v = self.reshape_mat(u,s,v)\n",
    "        \n",
    "        self.weight1_hat,self.v1 = self.new_weights(u,s,v)\n",
    "        self.v1=Parameter(self.v1,requires_grad=True)\n",
    "        self.weight1_hat=Parameter(self.weight1_hat,requires_grad=True)\n",
    "        \n",
    "        #weight2\n",
    "        self.weight2 = Variable(params[\"fc2.weight\"].t())\n",
    "        self.bias2 = Variable(params[\"fc2.bias\"])\n",
    "        \n",
    "        #Network compression\n",
    "        u,s,v = torch.svd(self.weight2)\n",
    "        u,s,v = self.reshape_mat(u,s,v)\n",
    "        \n",
    "        self.weight2_hat,self.v2 = self.new_weights(u,s,v)\n",
    "        self.v2=Parameter(self.v2,requires_grad=True)\n",
    "        self.weight2_hat=Parameter(self.weight2_hat,requires_grad=True)\n",
    "        \n",
    "        #weight3\n",
    "        self.weight3 = Variable(params[\"fc3.weight\"].t())\n",
    "        self.bias3 = Variable(params[\"fc3.bias\"])\n",
    "        \n",
    "        #Network compression\n",
    "        \n",
    "        u,s,v = torch.svd(self.weight3)\n",
    "        u,s,v = self.reshape_mat(u,s,v)\n",
    "        \n",
    "        self.weight3_hat,self.v3 = self.new_weights(u,s,v)\n",
    "        self.v3=Parameter(self.v3,requires_grad=True)\n",
    "        self.weight3_hat=Parameter(self.weight3_hat,requires_grad=True)\n",
    "        \n",
    "        #weight4\n",
    "        self.weight4 = Variable(params[\"fc4.weight\"].t())\n",
    "        self.bias4 = Variable(params[\"fc4.bias\"])\n",
    "        \n",
    "        #Network compression\n",
    "        \n",
    "        u,s,v = torch.svd(self.weight4)\n",
    "\n",
    "        \n",
    "        u,s,v = self.reshape_mat(u,s,v)\n",
    "        \n",
    "        self.weight4_hat ,self.v4 = self.new_weights(u,s,v)\n",
    "        self.v4=Parameter(self.v4,requires_grad=True)\n",
    "        self.weight4_hat=Parameter(self.weight4_hat,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        #weight5\n",
    "        self.weight5 = Variable(params[\"fc5.weight\"].t())\n",
    "        self.bias5 = Variable(params[\"fc5.bias\"])\n",
    "        \n",
    "        #Network compression\n",
    "        \n",
    "        u,s,v = torch.svd(self.weight5)\n",
    "        u,s,v = self.reshape_mat(u,s,v)\n",
    "        \n",
    "        self.weight5_hat ,self.v5 = self.new_weights(u,s,v)\n",
    "        self.v5=Parameter(self.v5,requires_grad=True)\n",
    "        self.weight5_hat=Parameter(self.weight5_hat,requires_grad=True)\n",
    "        \n",
    "        #weight6\n",
    "        self.weight6 = Variable(params[\"fc6.weight\"].t())\n",
    "        self.bias6 = Variable(params[\"fc6.bias\"])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        hidden_1 = x.mm(self.weight1_hat).mm(self.v1)\n",
    "        hidden_1 = hidden_1.add(self.bias1)\n",
    "        hidden_1_relu = hidden_1.clamp(min=0)\n",
    "        \n",
    "        \n",
    "        hidden_2 = hidden_1_relu.mm(self.weight2_hat).mm(self.v2)\n",
    "        hidden_2 = hidden_2.add(self.bias2)\n",
    "        hidden_2_relu = hidden_2.clamp(min=0)\n",
    "        \n",
    "        hidden_3 = hidden_2_relu.mm(self.weight3_hat).mm(self.v3)\n",
    "        hidden_3 = hidden_3.add(self.bias3)\n",
    "        hidden_3_relu = hidden_3.clamp(min=0)\n",
    "        \n",
    "        hidden_4 = hidden_3_relu.mm(self.weight4_hat).mm(self.v4)\n",
    "        hidden_4 = hidden_4.add(self.bias4)\n",
    "        hidden_4_relu = hidden_4.clamp(min=0)\n",
    "        \n",
    "        hidden_5 = hidden_4_relu.mm(self.weight5_hat).mm(self.v5)\n",
    "        hidden_5 = hidden_5.add(self.bias5)\n",
    "        hidden_5_relu = hidden_5.clamp(min=0)\n",
    "        \n",
    "        \n",
    "        last_layer = hidden_5_relu.mm(self.weight6)\n",
    "        last_layer = last_layer.add(self.bias6)\n",
    "\n",
    "        return F.log_softmax(last_layer,dim=1)\n",
    "    \n",
    "    def reshape_mat(self,u,s,v):\n",
    "        u = u[:,:self.D]\n",
    "         \n",
    "        s = s[:self.D]\n",
    "        \n",
    "        v = v[:,:self.D]\n",
    "        \n",
    "        return u,s,v\n",
    "    \n",
    "    def new_weights(self,u,s,v):\n",
    "        \n",
    "#         return torch.mm(u,torch.diag(s)).data,v.t().data\n",
    "        return u.data,torch.mm(torch.diag(s),v.t()).data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Test_Model(model)\n",
    "criterion2 = torch.nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_rank_test():\n",
    "    \n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "\n",
    "    for i,(data,target) in enumerate(test_loader):\n",
    "        data = data.view(-1,28*28)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "        data,target = Variable(data),Variable(target)\n",
    "        \n",
    "        y_pred = model2(data)\n",
    "        \n",
    "        \n",
    "        test_loss += criterion2(y_pred, target).data[0]\n",
    "        pred = y_pred.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.001f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / len(test_loader.dataset)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 9396/10000 (94.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_rank_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code to save best weights\n",
    "# torch.save(model.state_dict(),\"optimum_wts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The svd model weights are given to the Model_Three network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_Three(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,base_model,svd_model):\n",
    "        super(Model_Three, self).__init__()\n",
    "        \n",
    "        \n",
    "        #weight1 and bias\n",
    "        params_one = base_model.state_dict()\n",
    "        params_two = svd_model.state_dict()\n",
    "        \n",
    "        #Get U and S.V^T from the models\n",
    "        self.weight1_hat,self.v1 = Parameter(params_two[\"weight1_hat\"],requires_grad=True),Parameter(params_two[\"v1\"],requires_grad=True)\n",
    "        self.weight2_hat,self.v2 = Parameter(params_two[\"weight2_hat\"],requires_grad=True),Parameter(params_two[\"v2\"],requires_grad=True)\n",
    "        self.weight3_hat,self.v3 = Parameter(params_two[\"weight3_hat\"],requires_grad=True),Parameter(params_two[\"v3\"],requires_grad=True)\n",
    "        self.weight4_hat,self.v4 = Parameter(params_two[\"weight4_hat\"],requires_grad=True),Parameter(params_two[\"v4\"],requires_grad=True)\n",
    "        self.weight5_hat,self.v5 = Parameter(params_two[\"weight5_hat\"],requires_grad=True),Parameter(params_two[\"v5\"],requires_grad=True)\n",
    "        #Final layer weight update is not needed. So we will not attach it to this models parameters\n",
    "        self.weight6 = params_one[\"fc6.weight\"].t()\n",
    "                                     \n",
    "                                     \n",
    "        self.bias1 = Parameter(params_one[\"fc1.bias\"],requires_grad=True)\n",
    "        self.bias2 = Parameter(params_one[\"fc2.bias\"],requires_grad=True)\n",
    "        self.bias3 = Parameter(params_one[\"fc3.bias\"],requires_grad=True)\n",
    "        self.bias4 = Parameter(params_one[\"fc4.bias\"],requires_grad=True)\n",
    "        self.bias5 = Parameter(params_one[\"fc5.bias\"],requires_grad=True)\n",
    "        self.bias6 = Parameter(params_one[\"fc6.bias\"],requires_grad=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        hidden_1 = x.mm(self.weight1_hat).mm(self.v1)\n",
    "        hidden_1 = hidden_1.add(self.bias1)\n",
    "        hidden_1_relu = hidden_1.clamp(min=0)\n",
    "        \n",
    "        \n",
    "        hidden_2 = hidden_1_relu.mm(self.weight2_hat).mm(self.v2)\n",
    "        hidden_2 = hidden_2.add(self.bias2)\n",
    "        hidden_2_relu = hidden_2.clamp(min=0)\n",
    "        \n",
    "        hidden_3 = hidden_2_relu.mm(self.weight3_hat).mm(self.v3)\n",
    "        hidden_3 = hidden_3.add(self.bias3)\n",
    "        hidden_3_relu = hidden_3.clamp(min=0)\n",
    "        \n",
    "        hidden_4 = hidden_3_relu.mm(self.weight4_hat).mm(self.v4)\n",
    "        hidden_4 = hidden_4.add(self.bias4)\n",
    "        hidden_4_relu = hidden_4.clamp(min=0)\n",
    "        \n",
    "        hidden_5 = hidden_4_relu.mm(self.weight5_hat).mm(self.v5)\n",
    "        hidden_5 = hidden_5.add(self.bias5)\n",
    "        hidden_5_relu = hidden_5.clamp(min=0)\n",
    "        \n",
    "        \n",
    "        last_layer = hidden_5_relu.mm(Variable(self.weight6))\n",
    "        last_layer = last_layer.add(self.bias6)\n",
    "\n",
    "        return F.log_softmax(last_layer,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Model_Three(model,model2)\n",
    "if torch.cuda.is_available():\n",
    "    model3=model3.cuda()\n",
    "criterion3 = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 6.a) \n",
    "# We have defined the model required for 6.a as Test_Model let us learn U and v_hat using back propagation\n",
    "\n",
    "def train_svd(epoch):\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data=data.view(-1,28*28)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "        data,target = Variable(data),Variable(target)\n",
    "        # Clears the gradients of all optimized Variables\n",
    "        \n",
    "        optimizer3.zero_grad()\n",
    "        y_pred = model3(data)\n",
    "        loss = criterion3(y_pred, target)\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "        if batch_idx % args.interval  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.193364\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.024252\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.044949\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.034078\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.032778\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.003654\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.029158\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.005115\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.010272\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.036393\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000195\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.003898\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.008223\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.001100\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.002011\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.009607\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.002691\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000453\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.006556\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000972\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.000671\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.000320\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.000983\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000078\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.000596\n"
     ]
    }
   ],
   "source": [
    "for t in range(25):\n",
    "    train_svd(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_svd():\n",
    "    model3.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "\n",
    "    for i,(data,target) in enumerate(test_loader):\n",
    "        data = data.view(-1,28*28)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "        data,target = Variable(data),Variable(target)\n",
    "        \n",
    "        y_pred = model3(data)\n",
    "        \n",
    "        \n",
    "        test_loss += criterion3(y_pred, target).data[0]\n",
    "        pred = y_pred.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.001f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / len(test_loader.dataset)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0012, Accuracy: 9797/10000 (98.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_svd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The test accuracies after BP the svd, is 98% ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
