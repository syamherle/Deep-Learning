{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Along with this file please include following files to avoid time being wasted \n",
    "\n",
    "\n",
    "#### rnn_model_optimum_wts for trained model weights\n",
    "\n",
    "\n",
    "## I have commented the pickle load code, please uncomment and use the pickle load code instead of loading data and training model to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import Parameter\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--disable-cuda', action='store_true',\n",
    "                    help='Disable CUDA')\n",
    "parser.add_argument('--interval',metavar='N',default=100)\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.disable_cuda and torch.cuda.is_available()\n",
    "\n",
    "print(args.cuda)\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please load all the required training data from part2_data.p instead of running below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_path ='/opt/e533/timit-homework/tr'\n",
    "audio_files_dirty = sorted([f for f in os.listdir(ip_path) if f.startswith('trx')])   # X files\n",
    "audio_files_clean = sorted([f for f in os.listdir(ip_path) if f.startswith('trs')])   # S files\n",
    "audio_files_noise = sorted([f for f in os.listdir(ip_path) if f.startswith('trn')])   # N fiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "for i in range(1200):\n",
    "    sn, sr=librosa.load(ip_path+'/'+audio_files_dirty[i], sr=None)\n",
    "    X.append(librosa.stft(sn, n_fft=1024, hop_length=512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mag =[]\n",
    "for i in range(len(X)):\n",
    "    X_mag.append(np.abs(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S =[]\n",
    "for i in range(1200):\n",
    "    sn, sr=librosa.load(ip_path+'/'+audio_files_clean[i], sr=None)\n",
    "    S.append(librosa.stft(sn, n_fft=1024, hop_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_mag =[]\n",
    "for i in range(len(S)):\n",
    "    S_mag.append(np.abs(S[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N =[]\n",
    "for i in range(1200):\n",
    "    sn, sr=librosa.load(ip_path+'/'+audio_files_noise[i], sr=None)\n",
    "    N.append(librosa.stft(sn, n_fft=1024, hop_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_mag =[]\n",
    "for i in range(len(N)):\n",
    "    N_mag.append(np.abs(N[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contructing IBM\n",
    "M=[[]]*1200\n",
    "for i in range(len(M)):\n",
    "    temp = np.zeros((S_mag[i].shape[0],S_mag[i].shape[1]))\n",
    "    \n",
    "    \n",
    "    for j in range(S_mag[i].shape[0]):\n",
    "        for k in range(S_mag[i].shape[1]):\n",
    "            if S_mag[i][j][k] > N_mag[i][j][k]:\n",
    "                temp[j][k] = 1\n",
    "            else:\n",
    "                temp[j][k] = 0\n",
    "    \n",
    "    M[i] =temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please run below cell to load all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = pickle.load(open( \"part2_data.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,X_mag,S,S_mag,N,N_mag,M = data_dict[\"X\"],data_dict[\"X_mag\"],data_dict[\"S\"],data_dict[\"S_mag\"],data_dict[\"N\"],data_dict[\"N_mag\"],data_dict[\"IBM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two layer GRU RNN is implemented with 1024 hidden dimension and input dimension of 513 and dropout of 0.2 and batch_first as true\n",
    "\n",
    "##### Since the batch first is true input and output will be BatchSize(10),Sequence(column),Input_dimension(513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 513\n",
    "HIDDEN_DIM =1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining GRU model for speech denoising\n",
    "\n",
    "class GRU_RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,layer,batch_first,dropout=0.20):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "        #Assigning hidden and input dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim =input_dim\n",
    "        self.layer=layer\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim,layer,batch_first=batch_first,dropout=dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "         \n",
    "            \n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        \n",
    "        return Variable(torch.zeros(self.layer, batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.hidden = self.init_hidden(x.shape[0])\n",
    "        if torch.cuda.is_available():\n",
    "            self.hidden = self.hidden.cuda()\n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "        \n",
    "\n",
    "        output = F.sigmoid(self.fc(output))\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU_RNN(INPUT_DIM,HIDDEN_DIM,2,batch_first=True)\n",
    "if torch.cuda.is_available():\n",
    "    model= model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    batch_size = 10\n",
    "    m = len(X_mag)\n",
    "    costs=[]\n",
    "    for epoch in range(200):\n",
    "        n_batch = int(math.ceil(m/batch_size))\n",
    "        for batch_idx in range(n_batch):\n",
    "            start, end = batch_idx * batch_size, (batch_idx + 1) * batch_size\n",
    "            \n",
    "            data = np.rollaxis(np.array(X_mag[start:end]),-1,1)\n",
    "            target = np.rollaxis(np.array(M[start:end]),-1,1)\n",
    "            \n",
    "            \n",
    "            data,target=Variable(torch.from_numpy(data)),Variable(torch.from_numpy(target).float())\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data,target = data.cuda(),target.cuda()\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            model.hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                model.hidden = model.hidden.cuda()\n",
    "            \n",
    "            y_pred =model(data)\n",
    "            \n",
    "            \n",
    "\n",
    "            loss = criterion(y_pred,target)\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "               \n",
    "\n",
    "            if batch_idx % args.interval  == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\\\n",
    "                      format(epoch, batch_idx * len(data), len(S_mag),\\\n",
    "                             100. * batch_idx *len(data) / len(S_mag), loss.data[0]))\n",
    "                costs.append(loss.data[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1200 (0%)]\tLoss: 0.250350\n",
      "Train Epoch: 0 [1000/1200 (83%)]\tLoss: 0.254101\n",
      "Train Epoch: 1 [0/1200 (0%)]\tLoss: 0.188254\n",
      "Train Epoch: 1 [1000/1200 (83%)]\tLoss: 0.242685\n",
      "Train Epoch: 2 [0/1200 (0%)]\tLoss: 0.177390\n",
      "Train Epoch: 2 [1000/1200 (83%)]\tLoss: 0.240135\n",
      "Train Epoch: 3 [0/1200 (0%)]\tLoss: 0.166241\n",
      "Train Epoch: 3 [1000/1200 (83%)]\tLoss: 0.231914\n",
      "Train Epoch: 4 [0/1200 (0%)]\tLoss: 0.158103\n",
      "Train Epoch: 4 [1000/1200 (83%)]\tLoss: 0.217390\n",
      "Train Epoch: 5 [0/1200 (0%)]\tLoss: 0.151975\n",
      "Train Epoch: 5 [1000/1200 (83%)]\tLoss: 0.204689\n",
      "Train Epoch: 6 [0/1200 (0%)]\tLoss: 0.147128\n",
      "Train Epoch: 6 [1000/1200 (83%)]\tLoss: 0.194997\n",
      "Train Epoch: 7 [0/1200 (0%)]\tLoss: 0.142891\n",
      "Train Epoch: 7 [1000/1200 (83%)]\tLoss: 0.183918\n",
      "Train Epoch: 8 [0/1200 (0%)]\tLoss: 0.139583\n",
      "Train Epoch: 8 [1000/1200 (83%)]\tLoss: 0.174233\n",
      "Train Epoch: 9 [0/1200 (0%)]\tLoss: 0.136440\n",
      "Train Epoch: 9 [1000/1200 (83%)]\tLoss: 0.166585\n",
      "Train Epoch: 10 [0/1200 (0%)]\tLoss: 0.135241\n",
      "Train Epoch: 10 [1000/1200 (83%)]\tLoss: 0.159651\n",
      "Train Epoch: 11 [0/1200 (0%)]\tLoss: 0.132818\n",
      "Train Epoch: 11 [1000/1200 (83%)]\tLoss: 0.152725\n",
      "Train Epoch: 12 [0/1200 (0%)]\tLoss: 0.130902\n",
      "Train Epoch: 12 [1000/1200 (83%)]\tLoss: 0.147422\n",
      "Train Epoch: 13 [0/1200 (0%)]\tLoss: 0.129657\n",
      "Train Epoch: 13 [1000/1200 (83%)]\tLoss: 0.144250\n",
      "Train Epoch: 14 [0/1200 (0%)]\tLoss: 0.128225\n",
      "Train Epoch: 14 [1000/1200 (83%)]\tLoss: 0.138328\n",
      "Train Epoch: 15 [0/1200 (0%)]\tLoss: 0.125833\n",
      "Train Epoch: 15 [1000/1200 (83%)]\tLoss: 0.138788\n",
      "Train Epoch: 16 [0/1200 (0%)]\tLoss: 0.124540\n",
      "Train Epoch: 16 [1000/1200 (83%)]\tLoss: 0.132669\n",
      "Train Epoch: 17 [0/1200 (0%)]\tLoss: 0.124201\n",
      "Train Epoch: 17 [1000/1200 (83%)]\tLoss: 0.132312\n",
      "Train Epoch: 18 [0/1200 (0%)]\tLoss: 0.122805\n",
      "Train Epoch: 18 [1000/1200 (83%)]\tLoss: 0.132349\n",
      "Train Epoch: 19 [0/1200 (0%)]\tLoss: 0.123107\n",
      "Train Epoch: 19 [1000/1200 (83%)]\tLoss: 0.127674\n",
      "Train Epoch: 20 [0/1200 (0%)]\tLoss: 0.121294\n",
      "Train Epoch: 20 [1000/1200 (83%)]\tLoss: 0.124670\n",
      "Train Epoch: 21 [0/1200 (0%)]\tLoss: 0.120744\n",
      "Train Epoch: 21 [1000/1200 (83%)]\tLoss: 0.121873\n",
      "Train Epoch: 22 [0/1200 (0%)]\tLoss: 0.120486\n",
      "Train Epoch: 22 [1000/1200 (83%)]\tLoss: 0.120099\n",
      "Train Epoch: 23 [0/1200 (0%)]\tLoss: 0.119213\n",
      "Train Epoch: 23 [1000/1200 (83%)]\tLoss: 0.119387\n",
      "Train Epoch: 24 [0/1200 (0%)]\tLoss: 0.118588\n",
      "Train Epoch: 24 [1000/1200 (83%)]\tLoss: 0.115887\n",
      "Train Epoch: 25 [0/1200 (0%)]\tLoss: 0.117573\n",
      "Train Epoch: 25 [1000/1200 (83%)]\tLoss: 0.114466\n",
      "Train Epoch: 26 [0/1200 (0%)]\tLoss: 0.117320\n",
      "Train Epoch: 26 [1000/1200 (83%)]\tLoss: 0.114535\n",
      "Train Epoch: 27 [0/1200 (0%)]\tLoss: 0.116282\n",
      "Train Epoch: 27 [1000/1200 (83%)]\tLoss: 0.113628\n",
      "Train Epoch: 28 [0/1200 (0%)]\tLoss: 0.115392\n",
      "Train Epoch: 28 [1000/1200 (83%)]\tLoss: 0.111366\n",
      "Train Epoch: 29 [0/1200 (0%)]\tLoss: 0.115518\n",
      "Train Epoch: 29 [1000/1200 (83%)]\tLoss: 0.112993\n",
      "Train Epoch: 30 [0/1200 (0%)]\tLoss: 0.114456\n",
      "Train Epoch: 30 [1000/1200 (83%)]\tLoss: 0.114462\n",
      "Train Epoch: 31 [0/1200 (0%)]\tLoss: 0.114806\n",
      "Train Epoch: 31 [1000/1200 (83%)]\tLoss: 0.109812\n",
      "Train Epoch: 32 [0/1200 (0%)]\tLoss: 0.113451\n",
      "Train Epoch: 32 [1000/1200 (83%)]\tLoss: 0.108398\n",
      "Train Epoch: 33 [0/1200 (0%)]\tLoss: 0.113892\n",
      "Train Epoch: 33 [1000/1200 (83%)]\tLoss: 0.106593\n",
      "Train Epoch: 34 [0/1200 (0%)]\tLoss: 0.113650\n",
      "Train Epoch: 34 [1000/1200 (83%)]\tLoss: 0.107280\n",
      "Train Epoch: 35 [0/1200 (0%)]\tLoss: 0.113219\n",
      "Train Epoch: 35 [1000/1200 (83%)]\tLoss: 0.106596\n",
      "Train Epoch: 36 [0/1200 (0%)]\tLoss: 0.112164\n",
      "Train Epoch: 36 [1000/1200 (83%)]\tLoss: 0.104864\n",
      "Train Epoch: 37 [0/1200 (0%)]\tLoss: 0.112337\n",
      "Train Epoch: 37 [1000/1200 (83%)]\tLoss: 0.107563\n",
      "Train Epoch: 38 [0/1200 (0%)]\tLoss: 0.112233\n",
      "Train Epoch: 38 [1000/1200 (83%)]\tLoss: 0.105598\n",
      "Train Epoch: 39 [0/1200 (0%)]\tLoss: 0.112450\n",
      "Train Epoch: 39 [1000/1200 (83%)]\tLoss: 0.102783\n",
      "Train Epoch: 40 [0/1200 (0%)]\tLoss: 0.111170\n",
      "Train Epoch: 40 [1000/1200 (83%)]\tLoss: 0.100124\n",
      "Train Epoch: 41 [0/1200 (0%)]\tLoss: 0.110968\n",
      "Train Epoch: 41 [1000/1200 (83%)]\tLoss: 0.102616\n",
      "Train Epoch: 42 [0/1200 (0%)]\tLoss: 0.112692\n",
      "Train Epoch: 42 [1000/1200 (83%)]\tLoss: 0.102951\n",
      "Train Epoch: 43 [0/1200 (0%)]\tLoss: 0.109437\n",
      "Train Epoch: 43 [1000/1200 (83%)]\tLoss: 0.099853\n",
      "Train Epoch: 44 [0/1200 (0%)]\tLoss: 0.109249\n",
      "Train Epoch: 44 [1000/1200 (83%)]\tLoss: 0.098380\n",
      "Train Epoch: 45 [0/1200 (0%)]\tLoss: 0.109200\n",
      "Train Epoch: 45 [1000/1200 (83%)]\tLoss: 0.098276\n",
      "Train Epoch: 46 [0/1200 (0%)]\tLoss: 0.109968\n",
      "Train Epoch: 46 [1000/1200 (83%)]\tLoss: 0.098644\n",
      "Train Epoch: 47 [0/1200 (0%)]\tLoss: 0.109283\n",
      "Train Epoch: 47 [1000/1200 (83%)]\tLoss: 0.099776\n",
      "Train Epoch: 48 [0/1200 (0%)]\tLoss: 0.110926\n",
      "Train Epoch: 48 [1000/1200 (83%)]\tLoss: 0.098624\n",
      "Train Epoch: 49 [0/1200 (0%)]\tLoss: 0.110059\n",
      "Train Epoch: 49 [1000/1200 (83%)]\tLoss: 0.097410\n",
      "Train Epoch: 50 [0/1200 (0%)]\tLoss: 0.111490\n",
      "Train Epoch: 50 [1000/1200 (83%)]\tLoss: 0.097025\n",
      "Train Epoch: 51 [0/1200 (0%)]\tLoss: 0.111238\n",
      "Train Epoch: 51 [1000/1200 (83%)]\tLoss: 0.095720\n",
      "Train Epoch: 52 [0/1200 (0%)]\tLoss: 0.111831\n",
      "Train Epoch: 52 [1000/1200 (83%)]\tLoss: 0.094981\n",
      "Train Epoch: 53 [0/1200 (0%)]\tLoss: 0.110659\n",
      "Train Epoch: 53 [1000/1200 (83%)]\tLoss: 0.096018\n",
      "Train Epoch: 54 [0/1200 (0%)]\tLoss: 0.111331\n",
      "Train Epoch: 54 [1000/1200 (83%)]\tLoss: 0.093948\n",
      "Train Epoch: 55 [0/1200 (0%)]\tLoss: 0.108705\n",
      "Train Epoch: 55 [1000/1200 (83%)]\tLoss: 0.092280\n",
      "Train Epoch: 56 [0/1200 (0%)]\tLoss: 0.109363\n",
      "Train Epoch: 56 [1000/1200 (83%)]\tLoss: 0.093168\n",
      "Train Epoch: 57 [0/1200 (0%)]\tLoss: 0.108851\n",
      "Train Epoch: 57 [1000/1200 (83%)]\tLoss: 0.093519\n",
      "Train Epoch: 58 [0/1200 (0%)]\tLoss: 0.107823\n",
      "Train Epoch: 58 [1000/1200 (83%)]\tLoss: 0.091465\n",
      "Train Epoch: 59 [0/1200 (0%)]\tLoss: 0.106675\n",
      "Train Epoch: 59 [1000/1200 (83%)]\tLoss: 0.091299\n",
      "Train Epoch: 60 [0/1200 (0%)]\tLoss: 0.105576\n",
      "Train Epoch: 60 [1000/1200 (83%)]\tLoss: 0.091378\n",
      "Train Epoch: 61 [0/1200 (0%)]\tLoss: 0.104553\n",
      "Train Epoch: 61 [1000/1200 (83%)]\tLoss: 0.090161\n",
      "Train Epoch: 62 [0/1200 (0%)]\tLoss: 0.105081\n",
      "Train Epoch: 62 [1000/1200 (83%)]\tLoss: 0.090597\n",
      "Train Epoch: 63 [0/1200 (0%)]\tLoss: 0.103685\n",
      "Train Epoch: 63 [1000/1200 (83%)]\tLoss: 0.089494\n",
      "Train Epoch: 64 [0/1200 (0%)]\tLoss: 0.102797\n",
      "Train Epoch: 64 [1000/1200 (83%)]\tLoss: 0.089008\n",
      "Train Epoch: 65 [0/1200 (0%)]\tLoss: 0.103691\n",
      "Train Epoch: 65 [1000/1200 (83%)]\tLoss: 0.088573\n",
      "Train Epoch: 66 [0/1200 (0%)]\tLoss: 0.102796\n",
      "Train Epoch: 66 [1000/1200 (83%)]\tLoss: 0.087762\n",
      "Train Epoch: 67 [0/1200 (0%)]\tLoss: 0.102532\n",
      "Train Epoch: 67 [1000/1200 (83%)]\tLoss: 0.087789\n",
      "Train Epoch: 68 [0/1200 (0%)]\tLoss: 0.102492\n",
      "Train Epoch: 68 [1000/1200 (83%)]\tLoss: 0.088281\n",
      "Train Epoch: 69 [0/1200 (0%)]\tLoss: 0.100905\n",
      "Train Epoch: 69 [1000/1200 (83%)]\tLoss: 0.087834\n",
      "Train Epoch: 70 [0/1200 (0%)]\tLoss: 0.101130\n",
      "Train Epoch: 70 [1000/1200 (83%)]\tLoss: 0.086482\n",
      "Train Epoch: 71 [0/1200 (0%)]\tLoss: 0.100526\n",
      "Train Epoch: 71 [1000/1200 (83%)]\tLoss: 0.086177\n",
      "Train Epoch: 72 [0/1200 (0%)]\tLoss: 0.098824\n",
      "Train Epoch: 72 [1000/1200 (83%)]\tLoss: 0.085987\n",
      "Train Epoch: 73 [0/1200 (0%)]\tLoss: 0.098695\n",
      "Train Epoch: 73 [1000/1200 (83%)]\tLoss: 0.085705\n",
      "Train Epoch: 74 [0/1200 (0%)]\tLoss: 0.098022\n",
      "Train Epoch: 74 [1000/1200 (83%)]\tLoss: 0.085281\n",
      "Train Epoch: 75 [0/1200 (0%)]\tLoss: 0.097530\n",
      "Train Epoch: 75 [1000/1200 (83%)]\tLoss: 0.084919\n",
      "Train Epoch: 76 [0/1200 (0%)]\tLoss: 0.097900\n",
      "Train Epoch: 76 [1000/1200 (83%)]\tLoss: 0.084730\n",
      "Train Epoch: 77 [0/1200 (0%)]\tLoss: 0.097362\n",
      "Train Epoch: 77 [1000/1200 (83%)]\tLoss: 0.084511\n",
      "Train Epoch: 78 [0/1200 (0%)]\tLoss: 0.097135\n",
      "Train Epoch: 78 [1000/1200 (83%)]\tLoss: 0.085134\n",
      "Train Epoch: 79 [0/1200 (0%)]\tLoss: 0.095969\n",
      "Train Epoch: 79 [1000/1200 (83%)]\tLoss: 0.084801\n",
      "Train Epoch: 80 [0/1200 (0%)]\tLoss: 0.096093\n",
      "Train Epoch: 80 [1000/1200 (83%)]\tLoss: 0.084190\n",
      "Train Epoch: 81 [0/1200 (0%)]\tLoss: 0.096173\n",
      "Train Epoch: 81 [1000/1200 (83%)]\tLoss: 0.083341\n",
      "Train Epoch: 82 [0/1200 (0%)]\tLoss: 0.095142\n",
      "Train Epoch: 82 [1000/1200 (83%)]\tLoss: 0.083764\n",
      "Train Epoch: 83 [0/1200 (0%)]\tLoss: 0.094777\n",
      "Train Epoch: 83 [1000/1200 (83%)]\tLoss: 0.082694\n",
      "Train Epoch: 84 [0/1200 (0%)]\tLoss: 0.094168\n",
      "Train Epoch: 84 [1000/1200 (83%)]\tLoss: 0.083334\n",
      "Train Epoch: 85 [0/1200 (0%)]\tLoss: 0.094224\n",
      "Train Epoch: 85 [1000/1200 (83%)]\tLoss: 0.085270\n",
      "Train Epoch: 86 [0/1200 (0%)]\tLoss: 0.094709\n",
      "Train Epoch: 86 [1000/1200 (83%)]\tLoss: 0.095386\n",
      "Train Epoch: 87 [0/1200 (0%)]\tLoss: 0.094704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 87 [1000/1200 (83%)]\tLoss: 0.087073\n",
      "Train Epoch: 88 [0/1200 (0%)]\tLoss: 0.094350\n",
      "Train Epoch: 88 [1000/1200 (83%)]\tLoss: 0.082201\n",
      "Train Epoch: 89 [0/1200 (0%)]\tLoss: 0.093664\n",
      "Train Epoch: 89 [1000/1200 (83%)]\tLoss: 0.081473\n",
      "Train Epoch: 90 [0/1200 (0%)]\tLoss: 0.092744\n",
      "Train Epoch: 90 [1000/1200 (83%)]\tLoss: 0.080710\n",
      "Train Epoch: 91 [0/1200 (0%)]\tLoss: 0.092135\n",
      "Train Epoch: 91 [1000/1200 (83%)]\tLoss: 0.080441\n",
      "Train Epoch: 92 [0/1200 (0%)]\tLoss: 0.091326\n",
      "Train Epoch: 92 [1000/1200 (83%)]\tLoss: 0.080273\n",
      "Train Epoch: 93 [0/1200 (0%)]\tLoss: 0.091280\n",
      "Train Epoch: 93 [1000/1200 (83%)]\tLoss: 0.080889\n",
      "Train Epoch: 94 [0/1200 (0%)]\tLoss: 0.090872\n",
      "Train Epoch: 94 [1000/1200 (83%)]\tLoss: 0.079993\n",
      "Train Epoch: 95 [0/1200 (0%)]\tLoss: 0.090913\n",
      "Train Epoch: 95 [1000/1200 (83%)]\tLoss: 0.079500\n",
      "Train Epoch: 96 [0/1200 (0%)]\tLoss: 0.090445\n",
      "Train Epoch: 96 [1000/1200 (83%)]\tLoss: 0.080036\n",
      "Train Epoch: 97 [0/1200 (0%)]\tLoss: 0.090493\n",
      "Train Epoch: 97 [1000/1200 (83%)]\tLoss: 0.079246\n",
      "Train Epoch: 98 [0/1200 (0%)]\tLoss: 0.090964\n",
      "Train Epoch: 98 [1000/1200 (83%)]\tLoss: 0.079001\n",
      "Train Epoch: 99 [0/1200 (0%)]\tLoss: 0.090159\n",
      "Train Epoch: 99 [1000/1200 (83%)]\tLoss: 0.079168\n",
      "Train Epoch: 100 [0/1200 (0%)]\tLoss: 0.090378\n",
      "Train Epoch: 100 [1000/1200 (83%)]\tLoss: 0.079144\n",
      "Train Epoch: 101 [0/1200 (0%)]\tLoss: 0.090119\n",
      "Train Epoch: 101 [1000/1200 (83%)]\tLoss: 0.079239\n",
      "Train Epoch: 102 [0/1200 (0%)]\tLoss: 0.089439\n",
      "Train Epoch: 102 [1000/1200 (83%)]\tLoss: 0.079966\n",
      "Train Epoch: 103 [0/1200 (0%)]\tLoss: 0.089893\n",
      "Train Epoch: 103 [1000/1200 (83%)]\tLoss: 0.078591\n",
      "Train Epoch: 104 [0/1200 (0%)]\tLoss: 0.090271\n",
      "Train Epoch: 104 [1000/1200 (83%)]\tLoss: 0.078256\n",
      "Train Epoch: 105 [0/1200 (0%)]\tLoss: 0.090584\n",
      "Train Epoch: 105 [1000/1200 (83%)]\tLoss: 0.078452\n",
      "Train Epoch: 106 [0/1200 (0%)]\tLoss: 0.088602\n",
      "Train Epoch: 106 [1000/1200 (83%)]\tLoss: 0.078261\n",
      "Train Epoch: 107 [0/1200 (0%)]\tLoss: 0.089262\n",
      "Train Epoch: 107 [1000/1200 (83%)]\tLoss: 0.077090\n",
      "Train Epoch: 108 [0/1200 (0%)]\tLoss: 0.088249\n",
      "Train Epoch: 108 [1000/1200 (83%)]\tLoss: 0.077394\n",
      "Train Epoch: 109 [0/1200 (0%)]\tLoss: 0.088312\n",
      "Train Epoch: 109 [1000/1200 (83%)]\tLoss: 0.077420\n",
      "Train Epoch: 110 [0/1200 (0%)]\tLoss: 0.088692\n",
      "Train Epoch: 110 [1000/1200 (83%)]\tLoss: 0.076977\n",
      "Train Epoch: 111 [0/1200 (0%)]\tLoss: 0.088247\n",
      "Train Epoch: 111 [1000/1200 (83%)]\tLoss: 0.077005\n",
      "Train Epoch: 112 [0/1200 (0%)]\tLoss: 0.087662\n",
      "Train Epoch: 112 [1000/1200 (83%)]\tLoss: 0.076146\n",
      "Train Epoch: 113 [0/1200 (0%)]\tLoss: 0.087040\n",
      "Train Epoch: 113 [1000/1200 (83%)]\tLoss: 0.076345\n",
      "Train Epoch: 114 [0/1200 (0%)]\tLoss: 0.086804\n",
      "Train Epoch: 114 [1000/1200 (83%)]\tLoss: 0.076137\n",
      "Train Epoch: 115 [0/1200 (0%)]\tLoss: 0.087588\n",
      "Train Epoch: 115 [1000/1200 (83%)]\tLoss: 0.076148\n",
      "Train Epoch: 116 [0/1200 (0%)]\tLoss: 0.086982\n",
      "Train Epoch: 116 [1000/1200 (83%)]\tLoss: 0.075831\n",
      "Train Epoch: 117 [0/1200 (0%)]\tLoss: 0.086729\n",
      "Train Epoch: 117 [1000/1200 (83%)]\tLoss: 0.075309\n",
      "Train Epoch: 118 [0/1200 (0%)]\tLoss: 0.086568\n",
      "Train Epoch: 118 [1000/1200 (83%)]\tLoss: 0.075396\n",
      "Train Epoch: 119 [0/1200 (0%)]\tLoss: 0.086526\n",
      "Train Epoch: 119 [1000/1200 (83%)]\tLoss: 0.074914\n",
      "Train Epoch: 120 [0/1200 (0%)]\tLoss: 0.086127\n",
      "Train Epoch: 120 [1000/1200 (83%)]\tLoss: 0.074596\n",
      "Train Epoch: 121 [0/1200 (0%)]\tLoss: 0.086740\n",
      "Train Epoch: 121 [1000/1200 (83%)]\tLoss: 0.075155\n",
      "Train Epoch: 122 [0/1200 (0%)]\tLoss: 0.086360\n",
      "Train Epoch: 122 [1000/1200 (83%)]\tLoss: 0.075001\n",
      "Train Epoch: 123 [0/1200 (0%)]\tLoss: 0.085316\n",
      "Train Epoch: 123 [1000/1200 (83%)]\tLoss: 0.074605\n",
      "Train Epoch: 124 [0/1200 (0%)]\tLoss: 0.085330\n",
      "Train Epoch: 124 [1000/1200 (83%)]\tLoss: 0.074596\n",
      "Train Epoch: 125 [0/1200 (0%)]\tLoss: 0.085830\n",
      "Train Epoch: 125 [1000/1200 (83%)]\tLoss: 0.074594\n",
      "Train Epoch: 126 [0/1200 (0%)]\tLoss: 0.086018\n",
      "Train Epoch: 126 [1000/1200 (83%)]\tLoss: 0.074299\n",
      "Train Epoch: 127 [0/1200 (0%)]\tLoss: 0.085862\n",
      "Train Epoch: 127 [1000/1200 (83%)]\tLoss: 0.073819\n",
      "Train Epoch: 128 [0/1200 (0%)]\tLoss: 0.084932\n",
      "Train Epoch: 128 [1000/1200 (83%)]\tLoss: 0.073354\n",
      "Train Epoch: 129 [0/1200 (0%)]\tLoss: 0.084342\n",
      "Train Epoch: 129 [1000/1200 (83%)]\tLoss: 0.073140\n",
      "Train Epoch: 130 [0/1200 (0%)]\tLoss: 0.083938\n",
      "Train Epoch: 130 [1000/1200 (83%)]\tLoss: 0.073225\n",
      "Train Epoch: 131 [0/1200 (0%)]\tLoss: 0.084069\n",
      "Train Epoch: 131 [1000/1200 (83%)]\tLoss: 0.074565\n",
      "Train Epoch: 132 [0/1200 (0%)]\tLoss: 0.083710\n",
      "Train Epoch: 132 [1000/1200 (83%)]\tLoss: 0.073150\n",
      "Train Epoch: 133 [0/1200 (0%)]\tLoss: 0.084069\n",
      "Train Epoch: 133 [1000/1200 (83%)]\tLoss: 0.073314\n",
      "Train Epoch: 134 [0/1200 (0%)]\tLoss: 0.084403\n",
      "Train Epoch: 134 [1000/1200 (83%)]\tLoss: 0.072826\n",
      "Train Epoch: 135 [0/1200 (0%)]\tLoss: 0.083174\n",
      "Train Epoch: 135 [1000/1200 (83%)]\tLoss: 0.072595\n",
      "Train Epoch: 136 [0/1200 (0%)]\tLoss: 0.083118\n",
      "Train Epoch: 136 [1000/1200 (83%)]\tLoss: 0.072486\n",
      "Train Epoch: 137 [0/1200 (0%)]\tLoss: 0.083202\n",
      "Train Epoch: 137 [1000/1200 (83%)]\tLoss: 0.072672\n",
      "Train Epoch: 138 [0/1200 (0%)]\tLoss: 0.083200\n",
      "Train Epoch: 138 [1000/1200 (83%)]\tLoss: 0.072652\n",
      "Train Epoch: 139 [0/1200 (0%)]\tLoss: 0.082871\n",
      "Train Epoch: 139 [1000/1200 (83%)]\tLoss: 0.071848\n",
      "Train Epoch: 140 [0/1200 (0%)]\tLoss: 0.082826\n",
      "Train Epoch: 140 [1000/1200 (83%)]\tLoss: 0.072054\n",
      "Train Epoch: 141 [0/1200 (0%)]\tLoss: 0.082652\n",
      "Train Epoch: 141 [1000/1200 (83%)]\tLoss: 0.071640\n",
      "Train Epoch: 142 [0/1200 (0%)]\tLoss: 0.082613\n",
      "Train Epoch: 142 [1000/1200 (83%)]\tLoss: 0.071684\n",
      "Train Epoch: 143 [0/1200 (0%)]\tLoss: 0.082253\n",
      "Train Epoch: 143 [1000/1200 (83%)]\tLoss: 0.072890\n",
      "Train Epoch: 144 [0/1200 (0%)]\tLoss: 0.082056\n",
      "Train Epoch: 144 [1000/1200 (83%)]\tLoss: 0.072160\n",
      "Train Epoch: 145 [0/1200 (0%)]\tLoss: 0.082069\n",
      "Train Epoch: 145 [1000/1200 (83%)]\tLoss: 0.071907\n",
      "Train Epoch: 146 [0/1200 (0%)]\tLoss: 0.082639\n",
      "Train Epoch: 146 [1000/1200 (83%)]\tLoss: 0.072694\n",
      "Train Epoch: 147 [0/1200 (0%)]\tLoss: 0.082652\n",
      "Train Epoch: 147 [1000/1200 (83%)]\tLoss: 0.072039\n",
      "Train Epoch: 148 [0/1200 (0%)]\tLoss: 0.083410\n",
      "Train Epoch: 148 [1000/1200 (83%)]\tLoss: 0.071895\n",
      "Train Epoch: 149 [0/1200 (0%)]\tLoss: 0.082960\n",
      "Train Epoch: 149 [1000/1200 (83%)]\tLoss: 0.071597\n",
      "Train Epoch: 150 [0/1200 (0%)]\tLoss: 0.081699\n",
      "Train Epoch: 150 [1000/1200 (83%)]\tLoss: 0.072573\n",
      "Train Epoch: 151 [0/1200 (0%)]\tLoss: 0.081026\n",
      "Train Epoch: 151 [1000/1200 (83%)]\tLoss: 0.070853\n",
      "Train Epoch: 152 [0/1200 (0%)]\tLoss: 0.080880\n",
      "Train Epoch: 152 [1000/1200 (83%)]\tLoss: 0.070758\n",
      "Train Epoch: 153 [0/1200 (0%)]\tLoss: 0.081041\n",
      "Train Epoch: 153 [1000/1200 (83%)]\tLoss: 0.070186\n",
      "Train Epoch: 154 [0/1200 (0%)]\tLoss: 0.080651\n",
      "Train Epoch: 154 [1000/1200 (83%)]\tLoss: 0.069715\n",
      "Train Epoch: 155 [0/1200 (0%)]\tLoss: 0.080435\n",
      "Train Epoch: 155 [1000/1200 (83%)]\tLoss: 0.069611\n",
      "Train Epoch: 156 [0/1200 (0%)]\tLoss: 0.080443\n",
      "Train Epoch: 156 [1000/1200 (83%)]\tLoss: 0.069971\n",
      "Train Epoch: 157 [0/1200 (0%)]\tLoss: 0.080179\n",
      "Train Epoch: 157 [1000/1200 (83%)]\tLoss: 0.069910\n",
      "Train Epoch: 158 [0/1200 (0%)]\tLoss: 0.080381\n",
      "Train Epoch: 158 [1000/1200 (83%)]\tLoss: 0.069678\n",
      "Train Epoch: 159 [0/1200 (0%)]\tLoss: 0.079670\n",
      "Train Epoch: 159 [1000/1200 (83%)]\tLoss: 0.069909\n",
      "Train Epoch: 160 [0/1200 (0%)]\tLoss: 0.079285\n",
      "Train Epoch: 160 [1000/1200 (83%)]\tLoss: 0.069060\n",
      "Train Epoch: 161 [0/1200 (0%)]\tLoss: 0.079625\n",
      "Train Epoch: 161 [1000/1200 (83%)]\tLoss: 0.069679\n",
      "Train Epoch: 162 [0/1200 (0%)]\tLoss: 0.079410\n",
      "Train Epoch: 162 [1000/1200 (83%)]\tLoss: 0.069205\n",
      "Train Epoch: 163 [0/1200 (0%)]\tLoss: 0.079606\n",
      "Train Epoch: 163 [1000/1200 (83%)]\tLoss: 0.069045\n",
      "Train Epoch: 164 [0/1200 (0%)]\tLoss: 0.079342\n",
      "Train Epoch: 164 [1000/1200 (83%)]\tLoss: 0.068742\n",
      "Train Epoch: 165 [0/1200 (0%)]\tLoss: 0.078958\n",
      "Train Epoch: 165 [1000/1200 (83%)]\tLoss: 0.069022\n",
      "Train Epoch: 166 [0/1200 (0%)]\tLoss: 0.078847\n",
      "Train Epoch: 166 [1000/1200 (83%)]\tLoss: 0.068976\n",
      "Train Epoch: 167 [0/1200 (0%)]\tLoss: 0.079142\n",
      "Train Epoch: 167 [1000/1200 (83%)]\tLoss: 0.068577\n",
      "Train Epoch: 168 [0/1200 (0%)]\tLoss: 0.079129\n",
      "Train Epoch: 168 [1000/1200 (83%)]\tLoss: 0.068499\n",
      "Train Epoch: 169 [0/1200 (0%)]\tLoss: 0.078932\n",
      "Train Epoch: 169 [1000/1200 (83%)]\tLoss: 0.068556\n",
      "Train Epoch: 170 [0/1200 (0%)]\tLoss: 0.078683\n",
      "Train Epoch: 170 [1000/1200 (83%)]\tLoss: 0.068468\n",
      "Train Epoch: 171 [0/1200 (0%)]\tLoss: 0.078956\n",
      "Train Epoch: 171 [1000/1200 (83%)]\tLoss: 0.068431\n",
      "Train Epoch: 172 [0/1200 (0%)]\tLoss: 0.078455\n",
      "Train Epoch: 172 [1000/1200 (83%)]\tLoss: 0.069185\n",
      "Train Epoch: 173 [0/1200 (0%)]\tLoss: 0.079043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 173 [1000/1200 (83%)]\tLoss: 0.069053\n",
      "Train Epoch: 174 [0/1200 (0%)]\tLoss: 0.078326\n",
      "Train Epoch: 174 [1000/1200 (83%)]\tLoss: 0.068076\n",
      "Train Epoch: 175 [0/1200 (0%)]\tLoss: 0.078521\n",
      "Train Epoch: 175 [1000/1200 (83%)]\tLoss: 0.068076\n",
      "Train Epoch: 176 [0/1200 (0%)]\tLoss: 0.078365\n",
      "Train Epoch: 176 [1000/1200 (83%)]\tLoss: 0.067774\n",
      "Train Epoch: 177 [0/1200 (0%)]\tLoss: 0.077970\n",
      "Train Epoch: 177 [1000/1200 (83%)]\tLoss: 0.067466\n",
      "Train Epoch: 178 [0/1200 (0%)]\tLoss: 0.077599\n",
      "Train Epoch: 178 [1000/1200 (83%)]\tLoss: 0.067598\n",
      "Train Epoch: 179 [0/1200 (0%)]\tLoss: 0.077554\n",
      "Train Epoch: 179 [1000/1200 (83%)]\tLoss: 0.067828\n",
      "Train Epoch: 180 [0/1200 (0%)]\tLoss: 0.077503\n",
      "Train Epoch: 180 [1000/1200 (83%)]\tLoss: 0.068156\n",
      "Train Epoch: 181 [0/1200 (0%)]\tLoss: 0.077324\n",
      "Train Epoch: 181 [1000/1200 (83%)]\tLoss: 0.067869\n",
      "Train Epoch: 182 [0/1200 (0%)]\tLoss: 0.077453\n",
      "Train Epoch: 182 [1000/1200 (83%)]\tLoss: 0.067664\n",
      "Train Epoch: 183 [0/1200 (0%)]\tLoss: 0.077204\n",
      "Train Epoch: 183 [1000/1200 (83%)]\tLoss: 0.068251\n",
      "Train Epoch: 184 [0/1200 (0%)]\tLoss: 0.077686\n",
      "Train Epoch: 184 [1000/1200 (83%)]\tLoss: 0.067563\n",
      "Train Epoch: 185 [0/1200 (0%)]\tLoss: 0.077556\n",
      "Train Epoch: 185 [1000/1200 (83%)]\tLoss: 0.066769\n",
      "Train Epoch: 186 [0/1200 (0%)]\tLoss: 0.077971\n",
      "Train Epoch: 186 [1000/1200 (83%)]\tLoss: 0.067109\n",
      "Train Epoch: 187 [0/1200 (0%)]\tLoss: 0.076864\n",
      "Train Epoch: 187 [1000/1200 (83%)]\tLoss: 0.066624\n",
      "Train Epoch: 188 [0/1200 (0%)]\tLoss: 0.077168\n",
      "Train Epoch: 188 [1000/1200 (83%)]\tLoss: 0.066694\n",
      "Train Epoch: 189 [0/1200 (0%)]\tLoss: 0.076916\n",
      "Train Epoch: 189 [1000/1200 (83%)]\tLoss: 0.066797\n",
      "Train Epoch: 190 [0/1200 (0%)]\tLoss: 0.076785\n",
      "Train Epoch: 190 [1000/1200 (83%)]\tLoss: 0.066569\n",
      "Train Epoch: 191 [0/1200 (0%)]\tLoss: 0.076461\n",
      "Train Epoch: 191 [1000/1200 (83%)]\tLoss: 0.066899\n",
      "Train Epoch: 192 [0/1200 (0%)]\tLoss: 0.076473\n",
      "Train Epoch: 192 [1000/1200 (83%)]\tLoss: 0.067100\n",
      "Train Epoch: 193 [0/1200 (0%)]\tLoss: 0.076755\n",
      "Train Epoch: 193 [1000/1200 (83%)]\tLoss: 0.066690\n",
      "Train Epoch: 194 [0/1200 (0%)]\tLoss: 0.076412\n",
      "Train Epoch: 194 [1000/1200 (83%)]\tLoss: 0.066400\n",
      "Train Epoch: 195 [0/1200 (0%)]\tLoss: 0.075906\n",
      "Train Epoch: 195 [1000/1200 (83%)]\tLoss: 0.066701\n",
      "Train Epoch: 196 [0/1200 (0%)]\tLoss: 0.076726\n",
      "Train Epoch: 196 [1000/1200 (83%)]\tLoss: 0.066645\n",
      "Train Epoch: 197 [0/1200 (0%)]\tLoss: 0.076288\n",
      "Train Epoch: 197 [1000/1200 (83%)]\tLoss: 0.066139\n",
      "Train Epoch: 198 [0/1200 (0%)]\tLoss: 0.075825\n",
      "Train Epoch: 198 [1000/1200 (83%)]\tLoss: 0.066089\n",
      "Train Epoch: 199 [0/1200 (0%)]\tLoss: 0.075633\n",
      "Train Epoch: 199 [1000/1200 (83%)]\tLoss: 0.066533\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to avoid training time please load the model from below cell to save time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained models\n",
    "# model.load_state_dict(torch.load(\"rnn_model_optimum_wts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To avoid time from loading all files for validation please load the validation data from part2_val_data.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Validation Data\n",
    "ip_path ='/opt/e533/timit-homework/v'\n",
    "\n",
    "val_audio_files_dirty = sorted([f for f in os.listdir(ip_path) if f.startswith('vx')])   # X files\n",
    "val_audio_files_clean = sorted([f for f in os.listdir(ip_path) if f.startswith('vs')])   # S files\n",
    "val_audio_files_noise = sorted([f for f in os.listdir(ip_path) if f.startswith('vn')])   # N fiels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X =[]\n",
    "for i in range(len(val_audio_files_dirty)):\n",
    "    sn, sr=librosa.load(ip_path+'/'+val_audio_files_dirty[i], sr=None)\n",
    "    val_X.append(librosa.stft(sn, n_fft=1024, hop_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X_mag =[]\n",
    "for i in range(len(val_X)):\n",
    "    val_X_mag.append(np.abs(val_X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_S =[]\n",
    "for i in range(len(val_audio_files_clean)):\n",
    "    sn, sr=librosa.load(ip_path+'/'+val_audio_files_clean[i], sr=None)\n",
    "    val_S.append(librosa.stft(sn, n_fft=1024, hop_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_S_mag =[]\n",
    "for i in range(len(val_S)):\n",
    "    val_S_mag.append(np.abs(val_S[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_N =[]\n",
    "for i in range(len(val_audio_files_noise)):\n",
    "    sn, sr=librosa.load(ip_path+'/'+val_audio_files_noise[i], sr=None)\n",
    "    val_N.append(librosa.stft(sn, n_fft=1024, hop_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_N_mag =[]\n",
    "for i in range(len(val_N)):\n",
    "    val_N_mag.append(np.abs(val_N[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_M=[[]]*len(val_S_mag)\n",
    "for i in range(len(val_M)):\n",
    "    temp = np.zeros((val_S_mag[i].shape[0],val_S_mag[i].shape[1]))\n",
    "    \n",
    "    \n",
    "    for j in range(val_S_mag[i].shape[0]):\n",
    "        for k in range(val_S_mag[i].shape[1]):\n",
    "            if val_S_mag[i][j][k] > val_N_mag[i][j][k]:\n",
    "                temp[j][k] = 1\n",
    "            else:\n",
    "                temp[j][k] = 0\n",
    "    \n",
    "    val_M[i] =temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_dict = pickle.load(open( \"part2_val_data.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_X,val_X_mag,val_S,val_S_mag,val_N,val_N_mag,val_M = val_data_dict[\"val_X\"],val_data_dict[\"val_X_mag\"],val_data_dict[\"val_S\"],val_data_dict[\"val_S_mag\"],val_data_dict[\"val_N\"],val_data_dict[\"val_N_mag\"],val_data_dict[\"val_IBM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model with validation data\n",
    "\n",
    "def val_data():\n",
    "    model.eval()\n",
    "    torch.manual_seed(42)\n",
    "    batch_size = 1\n",
    "    m = len(val_X_mag)\n",
    "    costs=[]\n",
    "    n_batch = int(math.ceil(m/batch_size))\n",
    "    signal_predictions=[]\n",
    "    for batch_idx in range(n_batch):\n",
    "        start, end = batch_idx * batch_size, (batch_idx + 1) * batch_size\n",
    "        val_X\n",
    "        data = np.rollaxis(np.array(val_X_mag[start:end]),-1,1)\n",
    "        data_2 = np.rollaxis(np.array(val_X[start:end]),-1,1)\n",
    "        \n",
    "        data=Variable(torch.from_numpy(data))\n",
    "\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        model.hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            model.hidden = model.hidden.cuda()\n",
    "\n",
    "        y_pred =model(data)\n",
    "        prod = y_pred.cpu().data.numpy() * data_2\n",
    "        \n",
    "        signal_predictions.append(prod[0].T)\n",
    "    \n",
    "    return signal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_s_hat = val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover the speech audio\n",
    "val_s_hat_recovered=[]\n",
    "for i in range(len(val_s_hat)):\n",
    "    \n",
    "    val_s_hat_recovered.append(librosa.istft(val_s_hat[i],hop_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform inverse on clean signal\n",
    "val_s_clean=[]\n",
    "for i in range(len(val_S)):\n",
    "    \n",
    "    val_s_clean.append(librosa.istft(val_S[i],hop_length=512))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Speech to Noise ratio calculation for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Speech to Noise Ratio : 10.307045745067427\n"
     ]
    }
   ],
   "source": [
    "numerator=0\n",
    "denominator=0\n",
    "for i in range(len(val_s_clean)):\n",
    "    numerator += (np.sum(np.square(val_s_clean[i])))\n",
    "    denominator += np.sum(np.square(np.subtract(val_s_clean[i] , val_s_hat_recovered[i])))\n",
    "\n",
    "print(\"The Speech to Noise Ratio : {}\".format(10*(np.log10(np.divide(numerator,denominator)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To avoid time from loading all test files, load it from part2_test_data.p file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting test data\n",
    "ip_path ='/opt/e533/timit-homework/te'\n",
    "\n",
    "test_audio_files_dirty = sorted([f for f in os.listdir(ip_path) if f.startswith('tex')])   # X files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X =[]\n",
    "sr_list=[]\n",
    "for i in range(len(test_audio_files_dirty)):\n",
    "    sn, sr=librosa.load(ip_path+'/'+test_audio_files_dirty[i], sr=None)\n",
    "    test_X.append(librosa.stft(sn, n_fft=1024, hop_length=512))\n",
    "    sr_list.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_mag =[]\n",
    "for i in range(len(test_X)):\n",
    "    test_X_mag.append(np.abs(test_X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_dict = pickle.load(open( \"part2_test_data.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = test_data_dict[\"test_X\"]\n",
    "# test_x_mag = test_data_dict[\"test_X_mag\"]\n",
    "# sr_list = test_data_dict[\"sr_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    model.eval()\n",
    "    torch.manual_seed(42)\n",
    "    batch_size = 1\n",
    "    m = len(test_x_mag)\n",
    "    costs=[]\n",
    "    n_batch = int(math.ceil(m/batch_size))\n",
    "    signal_predictions=[]\n",
    "    for batch_idx in range(n_batch):\n",
    "        start, end = batch_idx * batch_size, (batch_idx +1 ) * batch_size\n",
    "        if start < len(test_x_mag):\n",
    "            \n",
    "\n",
    "            data = np.rollaxis(np.array(test_x_mag[start:end]),-1,1)\n",
    "            data_2 = np.rollaxis(np.array(test_x[start:end]),-1,1)\n",
    "            \n",
    "            data=Variable(torch.from_numpy(data))\n",
    "\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "\n",
    "            model.hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model.hidden = model.hidden.cuda()\n",
    "\n",
    "            y_pred =model(data)\n",
    "\n",
    "            prod = y_pred.cpu().data.numpy() * data_2\n",
    "\n",
    "            signal_predictions.append(prod[0].T)\n",
    "    \n",
    "    return signal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicted_test)):\n",
    "    \n",
    "    audio_spect = librosa.istft(predicted_test[i],hop_length=512)\n",
    "    librosa.output.write_wav('test_audio/test_'+str(i)+'.wav', audio_spect,sr_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
